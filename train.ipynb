{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Training Notebook: RetinaNet + U-Net\n",
    "\n",
    "This notebook provides training implementations for both RetinaNet (object detection) and U-Net (semantic segmentation) models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import copy\n",
    "import random\n",
    "import collections\n",
    "from natsort import natsorted\n",
    "from tqdm import trange\n",
    "from PIL import Image\n",
    "\n",
    "# Check GPU availability\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU count: {}'.format(torch.cuda.device_count()))\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RetinaNet Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetinaNet Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RetinaNet imports\n",
    "from retinanet import model\n",
    "from retinanet.dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, Normalizer\n",
    "from retinanet import coco_eval, csv_eval\n",
    "\n",
    "# RetinaNet Configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "RETINANET_CONFIG = {\n",
    "    'LEARNING_RATE': 1e-5,\n",
    "    'EPOCHS': 100,\n",
    "    'BATCH_SIZE': 4,\n",
    "    'DEPTH': 50,  # ResNet depth (18, 34, 50, 101, 152)\n",
    "    'PRETRAINED': True,\n",
    "    'MODEL_SAVE_PATH': './retinanet_weights/',\n",
    "    'DATASET_TYPE': 'csv'  # or 'coco'\n",
    "}\n",
    "\n",
    "# Create save directory\n",
    "if not os.path.exists(RETINANET_CONFIG['MODEL_SAVE_PATH']):\n",
    "    os.makedirs(RETINANET_CONFIG['MODEL_SAVE_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net imports\n",
    "from Unet.trainer import train, val\n",
    "from Unet.loss import dice_loss, dice\n",
    "from Unet.Unet import UNet\n",
    "from Unet.preprocessing import *\n",
    "from Unet.datagenerater import Dental_Single_Data_Generator\n",
    "from Unet.utils import *\n",
    "from Unet.progressbar import Bar\n",
    "\n",
    "# U-Net Configuration\n",
    "UNET_CONFIG = {\n",
    "    'IMAGE_SIZE': (512, 512),\n",
    "    'N_CLASSES': 14,  # Updated for 14 landmark points\n",
    "    'TRAIN_BATCH': 4,\n",
    "    'TEST_BATCH': 1,\n",
    "    'EPOCHS': 50,\n",
    "    'LEARNING_RATE': 5e-4,\n",
    "    'SEED': 42,\n",
    "    'MODEL_SAVE_PATH': './unet_weights/',\n",
    "    'ENCODER_NAME': 'vgg16',  # or 'timm-tf_efficientnet_lite4'\n",
    "    'USE_ATTENTION': True,\n",
    "    'NUM_LANDMARKS': 14  # Total number of landmark points\n",
    "}\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(UNET_CONFIG['SEED'])\n",
    "torch.manual_seed(UNET_CONFIG['SEED'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(UNET_CONFIG['SEED'])\n",
    "\n",
    "# Create save directory\n",
    "if not os.path.exists(UNET_CONFIG['MODEL_SAVE_PATH']):\n",
    "    os.makedirs(UNET_CONFIG['MODEL_SAVE_PATH'])\n",
    "\n",
    "print(f\"U-Net configuration set with image size: {UNET_CONFIG['IMAGE_SIZE']}\")\n",
    "print(f\"Number of landmark classes: {UNET_CONFIG['N_CLASSES']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your dataset paths here\n",
    "RETINANET_DATA_PATHS = {\n",
    "    'csv_train': './dataset/annotations_train.csv',\n",
    "    'csv_val': './dataset/annotations_val.csv',\n",
    "    'csv_classes': './dataset/classes.csv',\n",
    "    'coco_path': './dataset/coco/'  # if using COCO format\n",
    "}\n",
    "\n",
    "# Update paths as needed for your dataset\n",
    "print(\"RetinaNet dataset paths configured. Update the paths above to match your dataset location.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unet_dataloaders(x_train, y_train, x_val, y_val, config):\n",
    "    \"\"\"\n",
    "    Create data loaders for U-Net training with support for multiple landmark points\n",
    "    \"\"\"\n",
    "    # Data transforms\n",
    "    transform_train = transforms.Compose([\n",
    "        Gamma_2D(),\n",
    "        Shift_2D(),\n",
    "        RandomBrightness(),\n",
    "        Rotation_2D(),\n",
    "        RandomSharp(),\n",
    "        RandomBlur(),\n",
    "        RandomNoise(),\n",
    "        Invert(),\n",
    "        RandomClahe(),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    transform_val = transforms.Compose([\n",
    "        ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Create datasets for each landmark point\n",
    "    train_loaders = []\n",
    "    val_loaders = []\n",
    "    \n",
    "    for landmark_idx in range(config['NUM_LANDMARKS']):\n",
    "        # Create datasets for each landmark\n",
    "        trainset = Dental_Single_Data_Generator(\n",
    "            config['IMAGE_SIZE'], x_train, y_train, \n",
    "            landmark_num=landmark_idx, mode=\"train\", transform=transform_train\n",
    "        )\n",
    "        \n",
    "        valset = Dental_Single_Data_Generator(\n",
    "            config['IMAGE_SIZE'], x_val, y_val, \n",
    "            landmark_num=landmark_idx, mode=\"train\", transform=transform_val\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        trainloader = DataLoader(trainset, batch_size=config['TRAIN_BATCH'], shuffle=True)\n",
    "        valloader = DataLoader(valset, batch_size=config['TEST_BATCH'], shuffle=False)\n",
    "        \n",
    "        train_loaders.append(trainloader)\n",
    "        val_loaders.append(valloader)\n",
    "    \n",
    "    return train_loaders, val_loaders\n",
    "\n",
    "# Alternative: Single multi-class dataset approach\n",
    "def create_unet_dataloaders_multiclass(x_train, y_train, x_val, y_val, config):\n",
    "    \"\"\"\n",
    "    Create data loaders for U-Net training with multi-class output\n",
    "    \"\"\"\n",
    "    # Data transforms\n",
    "    transform_train = transforms.Compose([\n",
    "        Gamma_2D(),\n",
    "        Shift_2D(),\n",
    "        RandomBrightness(),\n",
    "        Rotation_2D(),\n",
    "        RandomSharp(),\n",
    "        RandomBlur(),\n",
    "        RandomNoise(),\n",
    "        Invert(),\n",
    "        RandomClahe(),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    transform_val = transforms.Compose([\n",
    "        ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Create datasets with all landmarks combined\n",
    "    trainset = Dental_Single_Data_Generator(\n",
    "        config['IMAGE_SIZE'], x_train, y_train, \n",
    "        landmark_num=-1, mode=\"train\", transform=transform_train  # -1 for all landmarks\n",
    "    )\n",
    "    \n",
    "    valset = Dental_Single_Data_Generator(\n",
    "        config['IMAGE_SIZE'], x_val, y_val, \n",
    "        landmark_num=-1, mode=\"train\", transform=transform_val\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    trainloader = DataLoader(trainset, batch_size=config['TRAIN_BATCH'], shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=config['TEST_BATCH'], shuffle=False)\n",
    "    \n",
    "    return trainloader, valloader\n",
    "\n",
    "# Create data loaders (uncomment when dataset is ready)\n",
    "# Option 1: Separate models for each landmark\n",
    "# train_loaders, val_loaders = create_unet_dataloaders(x_train, y_train, x_val, y_val, UNET_CONFIG)\n",
    "\n",
    "# Option 2: Single multi-class model\n",
    "# trainloader, valloader = create_unet_dataloaders_multiclass(x_train, y_train, x_val, y_val, UNET_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retinanet_dataloaders(config, data_paths):\n",
    "    if config['DATASET_TYPE'] == 'csv':\n",
    "        # CSV Dataset\n",
    "        dataset_train = CSVDataset(\n",
    "            train_file=data_paths['csv_train'],\n",
    "            class_list=data_paths['csv_classes'],\n",
    "            transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()])\n",
    "        )\n",
    "        \n",
    "        dataset_val = CSVDataset(\n",
    "            train_file=data_paths['csv_val'],\n",
    "            class_list=data_paths['csv_classes'],\n",
    "            transform=transforms.Compose([Normalizer(), Resizer()])\n",
    "        )\n",
    "        \n",
    "    elif config['DATASET_TYPE'] == 'coco':\n",
    "        # COCO Dataset\n",
    "        dataset_train = CocoDataset(\n",
    "            data_paths['coco_path'],\n",
    "            set_name='train2017',\n",
    "            transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()])\n",
    "        )\n",
    "        \n",
    "        dataset_val = CocoDataset(\n",
    "            data_paths['coco_path'],\n",
    "            set_name='val2017',\n",
    "            transform=transforms.Compose([Normalizer(), Resizer()])\n",
    "        )\n",
    "    \n",
    "    # Create data loaders\n",
    "    sampler_train = AspectRatioBasedSampler(dataset_train, batch_size=config['BATCH_SIZE'], drop_last=False)\n",
    "    dataloader_train = DataLoader(dataset_train, num_workers=0, collate_fn=collater, batch_sampler=sampler_train)\n",
    "    \n",
    "    sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n",
    "    dataloader_val = DataLoader(dataset_val, num_workers=0, collate_fn=collater, batch_sampler=sampler_val)\n",
    "    \n",
    "    return dataloader_train, dataloader_val, dataset_train, dataset_val\n",
    "\n",
    "# Create data loaders (uncomment when you have dataset paths configured)\n",
    "# dataloader_train, dataloader_val, dataset_train, dataset_val = create_retinanet_dataloaders(RETINANET_CONFIG, RETINANET_DATA_PATHS)\n",
    "# print(f'Num training images: {len(dataset_train)}')\n",
    "# print(f'Num validation images: {len(dataset_val)}')\n",
    "# print(f'Num classes: {dataset_train.num_classes()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet_multiple_landmarks(train_loaders, val_loaders, config):\n",
    "    \"\"\"\n",
    "    Train separate U-Net models for each landmark point (following the original approach)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    models = []\n",
    "    training_histories = []\n",
    "    \n",
    "    for landmark_idx in range(config['NUM_LANDMARKS']):\\n        print(f\\\"\\\\n{'='*60}\\\")\\n        print(f\\\"Training U-Net for Landmark {landmark_idx + 1}/{config['NUM_LANDMARKS']}\\\")\\n        print(f\\\"{'='*60}\\\")\\n        \\n        # Create model for this landmark\\n        model = create_unet_model(config)\\n        model.to(device)\\n        \\n        # Get data loaders for this landmark\\n        trainloader = train_loaders[landmark_idx]\\n        valloader = val_loaders[landmark_idx]\\n        \\n        # Optimizer\\n        optimizer = torch.optim.Adam(model.parameters(), lr=config['LEARNING_RATE'])\\n        \\n        # Training history\\n        train_losses = []\\n        val_losses = []\\n        best_val_loss = float('inf')\\n        \\n        for epoch in range(config['EPOCHS']):\\n            # Training phase\\n            model.train()\\n            epoch_train_losses = []\\n            epoch_train_dice = []\\n            \\n            for batch_idx, sample in enumerate(trainloader):\\n                images = sample['image'].to(device)\\n                masks = sample['landmarks'].to(device)\\n                \\n                # Forward pass\\n                outputs = model(images)\\n                outputs = torch.sigmoid(outputs)\\n                \\n                # Calculate loss\\n                loss = dice_loss(outputs, masks)\\n                dice_score = dice(outputs, masks)\\n                \\n                # Backward pass\\n                optimizer.zero_grad()\\n                loss.backward()\\n                optimizer.step()\\n                \\n                epoch_train_losses.append(loss.item())\\n                epoch_train_dice.append(dice_score.item())\\n            \\n            # Validation phase\\n            model.eval()\\n            epoch_val_losses = []\\n            epoch_val_dice = []\\n            \\n            with torch.no_grad():\\n                for batch_idx, sample in enumerate(valloader):\\n                    images = sample['image'].to(device)\\n                    masks = sample['landmarks'].to(device)\\n                    \\n                    outputs = model(images)\\n                    outputs = torch.sigmoid(outputs)\\n                    \\n                    loss = dice_loss(outputs, masks)\\n                    dice_score = dice(outputs, masks)\\n                    \\n                    epoch_val_losses.append(loss.item())\\n                    epoch_val_dice.append(dice_score.item())\\n            \\n            # Calculate epoch metrics\\n            avg_train_loss = np.mean(epoch_train_losses)\\n            avg_train_dice = np.mean(epoch_train_dice)\\n            avg_val_loss = np.mean(epoch_val_losses)\\n            avg_val_dice = np.mean(epoch_val_dice)\\n            \\n            train_losses.append(avg_train_loss)\\n            val_losses.append(avg_val_loss)\\n            \\n            if epoch % 5 == 0 or epoch == config['EPOCHS'] - 1:\\n                print(f'Epoch {epoch+1}/{config[\\\"EPOCHS\\\"]}:')\\n                print(f'  Train - Loss: {avg_train_loss:.5f}, Dice: {avg_train_dice:.5f}')\\n                print(f'  Val   - Loss: {avg_val_loss:.5f}, Dice: {avg_val_dice:.5f}')\\n            \\n            # Save best model\\n            if avg_val_loss < best_val_loss:\\n                best_val_loss = avg_val_loss\\n                \\n                # Create landmark-specific directory\\n                landmark_dir = os.path.join(config['MODEL_SAVE_PATH'], str(landmark_idx))\\n                if not os.path.exists(landmark_dir):\\n                    os.makedirs(landmark_dir)\\n                \\n                # Save model state dict\\n                if hasattr(model, 'module'):\\n                    state_dict = model.module.state_dict()\\n                else:\\n                    state_dict = model.state_dict()\\n                \\n                save_path = os.path.join(landmark_dir, 'weight.pth')\\n                torch.save(state_dict, save_path)\\n                \\n                if epoch % 5 == 0:\\n                    print(f'  Best model saved: {save_path} (Val Loss: {best_val_loss:.5f})')\\n        \\n        print(f'Landmark {landmark_idx + 1} training completed! Best val loss: {best_val_loss:.5f}')\\n        \\n        models.append(model)\\n        training_histories.append({\\n            'train_losses': train_losses,\\n            'val_losses': val_losses,\\n            'best_val_loss': best_val_loss\\n        })\\n    \\n    return models, training_histories\\n\\n\\ndef train_unet_multiclass(trainloader, valloader, config):\\n    \\\"\\\"\\\"\\n    Train single U-Net model for all landmark points (multi-class approach)\\n    \\\"\\\"\\\"\\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n    \\n    # Create model\\n    model = create_unet_model(config)\\n    model.to(device)\\n    \\n    # Optimizer\\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['LEARNING_RATE'])\\n    \\n    # Training history\\n    train_losses = []\\n    val_losses = []\\n    best_val_loss = float('inf')\\n    \\n    print(f\\\"Training multi-class U-Net for {config['NUM_LANDMARKS']} landmarks\\\")\\n    print(f\\\"{'='*60}\\\")\\n    \\n    for epoch in range(config['EPOCHS']):\\n        # Training phase\\n        model.train()\\n        epoch_train_losses = []\\n        epoch_train_dice = []\\n        \\n        for batch_idx, sample in enumerate(trainloader):\\n            images = sample['image'].to(device)\\n            masks = sample['landmarks'].to(device)\\n            \\n            # Forward pass\\n            outputs = model(images)\\n            outputs = torch.sigmoid(outputs)\\n            \\n            # Calculate loss (assuming masks have shape [batch, num_landmarks, height, width])\\n            loss = dice_loss(outputs, masks)\\n            dice_score = dice(outputs, masks)\\n            \\n            # Backward pass\\n            optimizer.zero_grad()\\n            loss.backward()\\n            optimizer.step()\\n            \\n            epoch_train_losses.append(loss.item())\\n            epoch_train_dice.append(dice_score.item())\\n        \\n        # Validation phase\\n        model.eval()\\n        epoch_val_losses = []\\n        epoch_val_dice = []\\n        \\n        with torch.no_grad():\\n            for batch_idx, sample in enumerate(valloader):\\n                images = sample['image'].to(device)\\n                masks = sample['landmarks'].to(device)\\n                \\n                outputs = model(images)\\n                outputs = torch.sigmoid(outputs)\\n                \\n                loss = dice_loss(outputs, masks)\\n                dice_score = dice(outputs, masks)\\n                \\n                epoch_val_losses.append(loss.item())\\n                epoch_val_dice.append(dice_score.item())\\n        \\n        # Calculate epoch metrics\\n        avg_train_loss = np.mean(epoch_train_losses)\\n        avg_train_dice = np.mean(epoch_train_dice)\\n        avg_val_loss = np.mean(epoch_val_losses)\\n        avg_val_dice = np.mean(epoch_val_dice)\\n        \\n        train_losses.append(avg_train_loss)\\n        val_losses.append(avg_val_loss)\\n        \\n        if epoch % 5 == 0 or epoch == config['EPOCHS'] - 1:\\n            print(f'Epoch {epoch+1}/{config[\\\"EPOCHS\\\"]}:')\\n            print(f'  Train - Loss: {avg_train_loss:.5f}, Dice: {avg_train_dice:.5f}')\\n            print(f'  Val   - Loss: {avg_val_loss:.5f}, Dice: {avg_val_dice:.5f}')\\n        \\n        # Save best model\\n        if avg_val_loss < best_val_loss:\\n            best_val_loss = avg_val_loss\\n            \\n            # Save model state dict\\n            if hasattr(model, 'module'):\\n                state_dict = model.module.state_dict()\\n            else:\\n                state_dict = model.state_dict()\\n            \\n            save_path = os.path.join(config['MODEL_SAVE_PATH'], 'best_multiclass_unet.pth')\\n            torch.save(state_dict, save_path)\\n            \\n            if epoch % 5 == 0:\\n                print(f'  Best model saved: {save_path} (Val Loss: {best_val_loss:.5f})')\\n    \\n    print(f'Multi-class training completed! Best validation loss: {best_val_loss:.5f}')\\n    \\n    return model, train_losses, val_losses\\n\\n\\n# Train U-Net models (uncomment when ready)\\n# Option 1: Train separate models for each landmark (original approach)\\n# models, histories = train_unet_multiple_landmarks(train_loaders, val_loaders, UNET_CONFIG)\\n\\n# Option 2: Train single multi-class model\\n# model, train_losses, val_losses = train_unet_multiclass(trainloader, valloader, UNET_CONFIG)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_unet_predictions(models, val_loaders, config, num_samples=2):\\n    \\\"\\\"\\\"\\n    Visualize U-Net predictions for multiple landmark models\\n    \\\"\\\"\\\"\\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n    \\n    # Set all models to eval mode\\n    for model in models:\\n        model.eval()\\n    \\n    with torch.no_grad():\\n        # Get a sample from the first landmark's validation set\\n        sample_data = next(iter(val_loaders[0]))\\n        images = sample_data['image'].to(device)\\n        \\n        for sample_idx in range(min(num_samples, images.shape[0])):\\n            # Create a grid for visualization\\n            num_cols = min(5, config['NUM_LANDMARKS'] + 1)  # Image + up to 4 landmarks per row\\n            num_rows = (config['NUM_LANDMARKS'] + num_cols) // num_cols\\n            \\n            fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 4, num_rows * 4))\\n            axes = axes.flatten() if num_rows > 1 else [axes] if num_cols == 1 else axes\\n            \\n            # Show original image\\n            image = images[sample_idx].cpu().numpy().squeeze()\\n            axes[0].imshow(image, cmap='gray')\\n            axes[0].set_title('Input Image')\\n            axes[0].axis('off')\\n            \\n            # Show predictions for each landmark\\n            for landmark_idx in range(config['NUM_LANDMARKS']):\\n                model = models[landmark_idx]\\n                output = model(images[sample_idx:sample_idx+1])\\n                prediction = torch.sigmoid(output).cpu().numpy().squeeze()\\n                \\n                ax_idx = landmark_idx + 1\\n                if ax_idx < len(axes):\\n                    axes[ax_idx].imshow(prediction, cmap='gray')\\n                    axes[ax_idx].set_title(f'Landmark {landmark_idx + 1}')\\n                    axes[ax_idx].axis('off')\\n            \\n            # Hide unused subplots\\n            for i in range(config['NUM_LANDMARKS'] + 1, len(axes)):\\n                axes[i].axis('off')\\n            \\n            plt.tight_layout()\\n            plt.show()\\n\\n\\ndef visualize_unet_multiclass_predictions(model, valloader, config, num_samples=2):\\n    \\\"\\\"\\\"\\n    Visualize U-Net predictions for multi-class model\\n    \\\"\\\"\\\"\\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n    model.eval()\\n    \\n    with torch.no_grad():\\n        for i, sample in enumerate(valloader):\\n            if i >= num_samples:\\n                break\\n            \\n            images = sample['image'].to(device)\\n            masks = sample['landmarks'].to(device)\\n            \\n            outputs = model(images)\\n            outputs = torch.sigmoid(outputs)\\n            \\n            # Convert to numpy for visualization\\n            image = images[0].cpu().numpy().squeeze()\\n            ground_truth = masks[0].cpu().numpy()  # Shape: [num_landmarks, height, width]\\n            predictions = outputs[0].cpu().numpy()  # Shape: [num_landmarks, height, width]\\n            \\n            # Create visualization grid\\n            num_cols = 4  # Image, GT, Pred, Overlay\\n            num_rows = config['NUM_LANDMARKS']\\n            \\n            fig, axes = plt.subplots(num_rows, num_cols, figsize=(16, num_rows * 4))\\n            \\n            for landmark_idx in range(config['NUM_LANDMARKS']):\\n                row = landmark_idx\\n                \\n                # Original image\\n                axes[row, 0].imshow(image, cmap='gray')\\n                axes[row, 0].set_title(f'Input Image (Landmark {landmark_idx + 1})')\\n                axes[row, 0].axis('off')\\n                \\n                # Ground truth\\n                axes[row, 1].imshow(ground_truth[landmark_idx], cmap='gray')\\n                axes[row, 1].set_title(f'Ground Truth {landmark_idx + 1}')\\n                axes[row, 1].axis('off')\\n                \\n                # Prediction\\n                axes[row, 2].imshow(predictions[landmark_idx], cmap='gray')\\n                axes[row, 2].set_title(f'Prediction {landmark_idx + 1}')\\n                axes[row, 2].axis('off')\\n                \\n                # Overlay\\n                overlay = image + predictions[landmark_idx]\\n                axes[row, 3].imshow(overlay, cmap='gray')\\n                axes[row, 3].set_title(f'Overlay {landmark_idx + 1}')\\n                axes[row, 3].axis('off')\\n            \\n            plt.tight_layout()\\n            plt.show()\\n\\n\\ndef plot_training_history_multiple_landmarks(histories, config):\\n    \\\"\\\"\\\"\\n    Plot training history for multiple landmark models\\n    \\\"\\\"\\\"\\n    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\\n    \\n    # Plot losses\\n    for i, history in enumerate(histories):\\n        axes[0].plot(history['train_losses'], label=f'Train Loss - Landmark {i+1}', alpha=0.7)\\n        axes[0].plot(history['val_losses'], label=f'Val Loss - Landmark {i+1}', alpha=0.7, linestyle='--')\\n    \\n    axes[0].set_title('Training and Validation Loss for All Landmarks')\\n    axes[0].set_xlabel('Epoch')\\n    axes[0].set_ylabel('Loss')\\n    axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\n    axes[0].grid(True, alpha=0.3)\\n    \\n    # Plot best validation losses\\n    best_losses = [h['best_val_loss'] for h in histories]\\n    axes[1].bar(range(1, config['NUM_LANDMARKS'] + 1), best_losses)\\n    axes[1].set_title('Best Validation Loss per Landmark')\\n    axes[1].set_xlabel('Landmark Index')\\n    axes[1].set_ylabel('Best Validation Loss')\\n    axes[1].grid(True, alpha=0.3)\\n    \\n    plt.tight_layout()\\n    plt.show()\\n    \\n    # Print summary\\n    print(\\\"\\\\nTraining Summary:\\\")\\n    print(\\\"=\\\" * 50)\\n    for i, history in enumerate(histories):\\n        print(f\\\"Landmark {i+1:2d}: Best Val Loss = {history['best_val_loss']:.5f}\\\")\\n    print(f\\\"\\\\nAverage Best Val Loss: {np.mean(best_losses):.5f}\\\")\\n    print(f\\\"Std Best Val Loss: {np.std(best_losses):.5f}\\\")\\n\\n\\n# Visualization functions (uncomment when models are trained)\\n# Option 1: Visualize multiple landmark models\\n# visualize_unet_predictions(models, val_loaders, UNET_CONFIG)\\n# plot_training_history_multiple_landmarks(histories, UNET_CONFIG)\\n\\n# Option 2: Visualize multi-class model\\n# visualize_unet_multiclass_predictions(model, valloader, UNET_CONFIG)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### To train RetinaNet:\n",
    "1. Update the `RETINANET_DATA_PATHS` dictionary with your dataset paths\n",
    "2. Uncomment the RetinaNet data loading and model creation cells\n",
    "3. Run the RetinaNet training cell\n",
    "\n",
    "### To train U-Net for 14 landmarks:\n",
    "\n",
    "**Option 1: Separate models for each landmark (Original approach)**\n",
    "- This approach trains 14 separate U-Net models, one for each landmark\n",
    "- Each model specializes in detecting a specific landmark point\n",
    "- Follows the original codebase structure\n",
    "\n",
    "**Option 2: Single multi-class model**\n",
    "- This approach trains one U-Net model that outputs all 14 landmarks simultaneously\n",
    "- More efficient in terms of memory and training time\n",
    "- Requires landmarks to be formatted as multi-channel masks\n",
    "\n",
    "### Steps to train U-Net:\n",
    "1. Update the `UNET_DATA_PATHS` dictionary with your dataset paths\n",
    "2. Choose your training approach (Option 1 or Option 2)\n",
    "3. Uncomment the appropriate data loading cells\n",
    "4. Run the U-Net training cell\n",
    "\n",
    "### Configuration Details:\n",
    "- **NUM_LANDMARKS**: Set to 14 for your landmark points\n",
    "- **N_CLASSES**: Set to 14 for multi-class approach, or 1 for individual landmark models\n",
    "- **IMAGE_SIZE**: Adjust based on your input image dimensions\n",
    "- **BATCH_SIZE**: Adjust based on your GPU memory\n",
    "\n",
    "### Model Saving:\n",
    "- **Separate models**: Saves each landmark model in `./unet_weights/{landmark_idx}/weight.pth`\n",
    "- **Multi-class model**: Saves in `./unet_weights/best_multiclass_unet.pth`\n",
    "\n",
    "### Notes:\n",
    "- Make sure you have the required dependencies installed\n",
    "- The original codebase uses the separate models approach (Option 1)\n",
    "- Adjust hyperparameters in the configuration dictionaries as needed\n",
    "- Monitor GPU memory usage and adjust batch sizes if necessary\n",
    "- Both approaches will save checkpoints during training\n",
    "\n",
    "### Expected Data Format:\n",
    "- Images: PNG format in the specified image directory\n",
    "- Labels: NumPy (.npy) format containing landmark annotations\n",
    "- The data generator expects landmark_num parameter to select specific landmarks (0-13 for individual landmarks, -1 for all landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_retinanet(retinanet, dataloader_train, dataloader_val, config, dataset_type='csv'):\n",
    "    retinanet.training = True\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = optim.Adam(retinanet.parameters(), lr=config['LEARNING_RATE'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "    \n",
    "    loss_hist = collections.deque(maxlen=500)\n",
    "    best_map = 0.0\n",
    "    \n",
    "    for epoch_num in range(config['EPOCHS']):\n",
    "        # Training phase\n",
    "        retinanet.train()\n",
    "        retinanet.module.freeze_bn()\n",
    "        \n",
    "        epoch_loss = []\n",
    "        \n",
    "        for iter_num, data in enumerate(dataloader_train):\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    classification_loss, regression_loss = retinanet([data['img'].cuda().float(), data['annot']])\n",
    "                else:\n",
    "                    classification_loss, regression_loss = retinanet([data['img'].float(), data['annot']])\n",
    "                \n",
    "                classification_loss = classification_loss.mean()\n",
    "                regression_loss = regression_loss.mean()\n",
    "                \n",
    "                loss = classification_loss + regression_loss\n",
    "                \n",
    "                if bool(loss == 0):\n",
    "                    continue\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.1)\n",
    "                optimizer.step()\n",
    "                \n",
    "                loss_hist.append(float(loss))\n",
    "                epoch_loss.append(float(loss))\n",
    "                \n",
    "                if iter_num % 10 == 0:\n",
    "                    print(f'Epoch: {epoch_num} | Iteration: {iter_num} | '\n",
    "                          f'Classification loss: {float(classification_loss):.5f} | '\n",
    "                          f'Regression loss: {float(regression_loss):.5f} | '\n",
    "                          f'Running loss: {np.mean(loss_hist):.5f}')\n",
    "                \n",
    "                del classification_loss, regression_loss\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'Error in training iteration: {e}')\n",
    "                continue\n",
    "        \n",
    "        # Validation phase\n",
    "        print('Evaluating dataset...')\n",
    "        \n",
    "        if dataset_type == 'coco':\n",
    "            coco_eval.evaluate_coco(dataloader_val, retinanet)\n",
    "        elif dataset_type == 'csv':\n",
    "            mAP = csv_eval.evaluate(dataloader_val, retinanet)\n",
    "            \n",
    "            # Calculate mean mAP\n",
    "            mean_map = np.mean([ap[0] for ap in mAP])\n",
    "            print(f'Mean mAP: {mean_map:.4f}')\n",
    "            \n",
    "            # Save best model\n",
    "            if mean_map > best_map:\n",
    "                best_map = mean_map\n",
    "                model_path = os.path.join(config['MODEL_SAVE_PATH'], f'best_retinanet_epoch_{epoch_num}.pt')\n",
    "                torch.save(retinanet.module, model_path)\n",
    "                print(f'Best model saved: {model_path} (mAP: {best_map:.4f})')\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(np.mean(epoch_loss))\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = os.path.join(config['MODEL_SAVE_PATH'], f'retinanet_epoch_{epoch_num}.pt')\n",
    "        torch.save(retinanet.module, checkpoint_path)\n",
    "    \n",
    "    # Save final model\n",
    "    retinanet.eval()\n",
    "    final_model_path = os.path.join(config['MODEL_SAVE_PATH'], 'retinanet_final.pt')\n",
    "    torch.save(retinanet, final_model_path)\n",
    "    print(f'Training completed! Final model saved: {final_model_path}')\n",
    "\n",
    "# Train RetinaNet (uncomment when ready)\n",
    "# train_retinanet(retinanet, dataloader_train, dataloader_val, RETINANET_CONFIG, RETINANET_CONFIG['DATASET_TYPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. U-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net imports\n",
    "from Unet.trainer import train, val\n",
    "from Unet.loss import dice_loss, dice\n",
    "from Unet.Unet import UNet\n",
    "from Unet.preprocessing import *\n",
    "from Unet.datagenerater import Dental_Single_Data_Generator\n",
    "from Unet.utils import *\n",
    "from Unet.progressbar import Bar\n",
    "\n",
    "# U-Net Configuration\n",
    "UNET_CONFIG = {\n",
    "    'IMAGE_SIZE': (512, 512),\n",
    "    'N_CLASSES': 1,\n",
    "    'TRAIN_BATCH': 4,\n",
    "    'TEST_BATCH': 1,\n",
    "    'EPOCHS': 50,\n",
    "    'LEARNING_RATE': 5e-4,\n",
    "    'SEED': 42,\n",
    "    'MODEL_SAVE_PATH': './unet_weights/',\n",
    "    'ENCODER_NAME': 'vgg16',  # or 'timm-tf_efficientnet_lite4'\n",
    "    'USE_ATTENTION': True\n",
    "}\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(UNET_CONFIG['SEED'])\n",
    "torch.manual_seed(UNET_CONFIG['SEED'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(UNET_CONFIG['SEED'])\n",
    "\n",
    "# Create save directory\n",
    "if not os.path.exists(UNET_CONFIG['MODEL_SAVE_PATH']):\n",
    "    os.makedirs(UNET_CONFIG['MODEL_SAVE_PATH'])\n",
    "\n",
    "print(f\"U-Net configuration set with image size: {UNET_CONFIG['IMAGE_SIZE']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your U-Net dataset paths\n",
    "UNET_DATA_PATHS = {\n",
    "    'image_path': './dataset/images/',\n",
    "    'label_path': './dataset/labels/',\n",
    "    'train_split': 0.8  # 80% for training, 20% for validation\n",
    "}\n",
    "\n",
    "def prepare_unet_dataset(data_paths):\n",
    "    \"\"\"\n",
    "    Prepare U-Net dataset by splitting images and labels into train/val sets\n",
    "    \"\"\"\n",
    "    # Get all image and label files\n",
    "    image_files = natsorted(glob.glob(os.path.join(data_paths['image_path'], '*.png')))\n",
    "    label_files = natsorted(glob.glob(os.path.join(data_paths['label_path'], '*.npy')))\n",
    "    \n",
    "    # Match image and label files\n",
    "    matched_pairs = []\n",
    "    for label_file in label_files:\n",
    "        base_name = os.path.basename(label_file).split('.')[0]\n",
    "        matching_images = [img for img in image_files if base_name in os.path.basename(img)]\n",
    "        if matching_images:\n",
    "            matched_pairs.append((matching_images[0], label_file))\n",
    "    \n",
    "    # Split into train/val\n",
    "    split_idx = int(len(matched_pairs) * data_paths['train_split'])\n",
    "    \n",
    "    train_pairs = matched_pairs[:split_idx]\n",
    "    val_pairs = matched_pairs[split_idx:]\n",
    "    \n",
    "    x_train = [pair[0] for pair in train_pairs]\n",
    "    y_train = [pair[1] for pair in train_pairs]\n",
    "    x_val = [pair[0] for pair in val_pairs]\n",
    "    y_val = [pair[1] for pair in val_pairs]\n",
    "    \n",
    "    print(f'Training samples: {len(x_train)}')\n",
    "    print(f'Validation samples: {len(x_val)}')\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "# Prepare dataset (uncomment when you have dataset paths configured)\n",
    "# x_train, y_train, x_val, y_val = prepare_unet_dataset(UNET_DATA_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Data Transforms and Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unet_dataloaders(x_train, y_train, x_val, y_val, config):\n",
    "    \"\"\"\n",
    "    Create data loaders for U-Net training\n",
    "    \"\"\"\n",
    "    # Data transforms\n",
    "    transform_train = transforms.Compose([\n",
    "        Gamma_2D(),\n",
    "        Shift_2D(),\n",
    "        RandomBrightness(),\n",
    "        Rotation_2D(),\n",
    "        RandomSharp(),\n",
    "        RandomBlur(),\n",
    "        RandomNoise(),\n",
    "        Invert(),\n",
    "        RandomClahe(),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    transform_val = transforms.Compose([\n",
    "        ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    trainset = Dental_Single_Data_Generator(\n",
    "        config['IMAGE_SIZE'], x_train, y_train, \n",
    "        landmark_num=0, mode=\"train\", transform=transform_train\n",
    "    )\n",
    "    \n",
    "    valset = Dental_Single_Data_Generator(\n",
    "        config['IMAGE_SIZE'], x_val, y_val, \n",
    "        landmark_num=0, mode=\"train\", transform=transform_val\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    trainloader = DataLoader(trainset, batch_size=config['TRAIN_BATCH'], shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=config['TEST_BATCH'], shuffle=False)\n",
    "    \n",
    "    return trainloader, valloader\n",
    "\n",
    "# Create data loaders (uncomment when dataset is ready)\n",
    "# trainloader, valloader = create_unet_dataloaders(x_train, y_train, x_val, y_val, UNET_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unet_model(config):\n",
    "    \"\"\"\n",
    "    Create U-Net model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to use segmentation_models_pytorch\n",
    "        import segmentation_models_pytorch as smp\n",
    "        \n",
    "        if config['USE_ATTENTION']:\n",
    "            model = smp.Unet(\n",
    "                encoder_name=config['ENCODER_NAME'],\n",
    "                decoder_attention_type='scse',\n",
    "                in_channels=1,\n",
    "                classes=config['N_CLASSES']\n",
    "            )\n",
    "        else:\n",
    "            model = smp.Unet(\n",
    "                encoder_name=config['ENCODER_NAME'],\n",
    "                in_channels=1,\n",
    "                classes=config['N_CLASSES']\n",
    "            )\n",
    "        \n",
    "        print(f'Created U-Net with {config[\"ENCODER_NAME\"]} encoder')\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback to basic U-Net implementation\n",
    "        model = UNet(n_channels=1, n_classes=config['N_CLASSES'])\n",
    "        print('Created basic U-Net model')\n",
    "    \n",
    "    # Load pretrained weights if available\n",
    "    weight_files = glob.glob(os.path.join(config['MODEL_SAVE_PATH'], '*.pth'))\n",
    "    if weight_files:\n",
    "        latest_weight = natsorted(weight_files)[-1]\n",
    "        try:\n",
    "            state_dict = torch.load(latest_weight)\n",
    "            model.load_state_dict(state_dict)\n",
    "            print(f'Loaded pretrained weights from: {latest_weight}')\n",
    "        except Exception as e:\n",
    "            print(f'Could not load weights: {e}')\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        model = model.cuda()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model (uncomment when ready)\n",
    "# unet_model = create_unet_model(UNET_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(model, trainloader, valloader, config):\n",
    "    \"\"\"\n",
    "    Train U-Net model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['LEARNING_RATE'])\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config['EPOCHS']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        epoch_train_dice = []\n",
    "        \n",
    "        for batch_idx, sample in enumerate(trainloader):\n",
    "            images = sample['image'].to(device)\n",
    "            masks = sample['landmarks'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = dice_loss(outputs, masks)\n",
    "            dice_score = dice(outputs, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_losses.append(loss.item())\n",
    "            epoch_train_dice.append(dice_score.item())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        epoch_val_dice = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, sample in enumerate(valloader):\n",
    "                images = sample['image'].to(device)\n",
    "                masks = sample['landmarks'].to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                \n",
    "                loss = dice_loss(outputs, masks)\n",
    "                dice_score = dice(outputs, masks)\n",
    "                \n",
    "                epoch_val_losses.append(loss.item())\n",
    "                epoch_val_dice.append(dice_score.item())\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_train_loss = np.mean(epoch_train_losses)\n",
    "        avg_train_dice = np.mean(epoch_train_dice)\n",
    "        avg_val_loss = np.mean(epoch_val_losses)\n",
    "        avg_val_dice = np.mean(epoch_val_dice)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{config[\"EPOCHS\"]}:')\n",
    "        print(f'  Train - Loss: {avg_train_loss:.5f}, Dice: {avg_train_dice:.5f}')\n",
    "        print(f'  Val   - Loss: {avg_val_loss:.5f}, Dice: {avg_val_dice:.5f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            \n",
    "            # Save model state dict\n",
    "            if hasattr(model, 'module'):\n",
    "                state_dict = model.module.state_dict()\n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "            \n",
    "            save_path = os.path.join(config['MODEL_SAVE_PATH'], 'best_unet.pth')\n",
    "            torch.save(state_dict, save_path)\n",
    "            print(f'  Best model saved: {save_path} (Val Loss: {best_val_loss:.5f})')\n",
    "        \n",
    "        print('-' * 50)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Val Loss')\n",
    "    plt.title('Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config['MODEL_SAVE_PATH'], 'training_history.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Training completed! Best validation loss: {best_val_loss:.5f}')\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Train U-Net (uncomment when ready)\n",
    "# trained_unet, train_losses, val_losses = train_unet(unet_model, trainloader, valloader, UNET_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_unet_predictions(model, dataloader, config, num_samples=4):\n",
    "    \"\"\"\n",
    "    Visualize U-Net predictions\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(dataloader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            \n",
    "            images = sample['image'].to(device)\n",
    "            masks = sample['landmarks'].to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            \n",
    "            # Convert to numpy for visualization\n",
    "            image = images[0].cpu().numpy().squeeze()\n",
    "            mask = masks[0].cpu().numpy().squeeze()\n",
    "            prediction = outputs[0].cpu().numpy().squeeze()\n",
    "            \n",
    "            # Create visualization\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "            \n",
    "            axes[0].imshow(image, cmap='gray')\n",
    "            axes[0].set_title('Input Image')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            axes[1].imshow(mask, cmap='gray')\n",
    "            axes[1].set_title('Ground Truth')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            axes[2].imshow(prediction, cmap='gray')\n",
    "            axes[2].set_title('Prediction')\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            # Overlay\n",
    "            overlay = image + prediction\n",
    "            axes[3].imshow(overlay, cmap='gray')\n",
    "            axes[3].set_title('Overlay')\n",
    "            axes[3].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Visualize predictions (uncomment when model is trained)\n",
    "# visualize_unet_predictions(trained_unet, valloader, UNET_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### To train RetinaNet:\n",
    "1. Update the `RETINANET_DATA_PATHS` dictionary with your dataset paths\n",
    "2. Uncomment the RetinaNet data loading and model creation cells\n",
    "3. Run the RetinaNet training cell\n",
    "\n",
    "### To train U-Net:\n",
    "1. Update the `UNET_DATA_PATHS` dictionary with your dataset paths\n",
    "2. Uncomment the U-Net data loading and model creation cells\n",
    "3. Run the U-Net training cell\n",
    "\n",
    "### Notes:\n",
    "- Make sure you have the required dependencies installed\n",
    "- Adjust hyperparameters in the configuration dictionaries as needed\n",
    "- Monitor GPU memory usage and adjust batch sizes if necessary\n",
    "- Both models will save checkpoints during training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
