{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Evaluation Notebook: RetinaNet + U-Net\n",
    "\n",
    "This notebook provides comprehensive inference and evaluation capabilities for both RetinaNet (object detection) and U-Net (landmark detection) models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RetinaNet Inference and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetinaNet Model Loading and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RetinaNet imports\n",
    "from retinanet import model\n",
    "from retinanet.dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Normalizer\n",
    "from retinanet import csv_eval, coco_eval\n",
    "from retinanet.utils import BBoxTransform, ClipBoxes\n",
    "\n",
    "# RetinaNet Inference Configuration\n",
    "RETINANET_CONFIG = {\n",
    "    'MODEL_PATH': './retinanet_weights/best_retinanet_epoch_50.pt',  # Update with your model path\n",
    "    'DATASET_TYPE': 'csv',  # 'csv' or 'coco'\n",
    "    'CONFIDENCE_THRESHOLD': 0.5,\n",
    "    'NMS_THRESHOLD': 0.5,\n",
    "    'MAX_DETECTIONS': 100,\n",
    "    'BATCH_SIZE': 1,\n",
    "    'IMAGE_SIZE': (512, 512),\n",
    "    'NUM_CLASSES': 1  # Update based on your dataset\n",
    "}\n",
    "\n",
    "# Dataset paths for evaluation\n",
    "RETINANET_DATA_PATHS = {\n",
    "    'csv_test': './dataset/annotations_test.csv',\n",
    "    'csv_classes': './dataset/classes.csv',\n",
    "    'coco_path': './dataset/coco/',\n",
    "    'test_images': './dataset/test_images/'\n",
    "}\n",
    "\n",
    "print(\"RetinaNet inference configuration loaded.\")\n",
    "print(f\"Model path: {RETINANET_CONFIG['MODEL_PATH']}\")\n",
    "print(f\"Confidence threshold: {RETINANET_CONFIG['CONFIDENCE_THRESHOLD']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetinaNet Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_retinanet_model(model_path, num_classes, device):\n",
    "    \"\"\"\n",
    "    Load RetinaNet model from checkpoint\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the model\n",
    "        model = torch.load(model_path, map_location=device)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        print(f\"RetinaNet model loaded successfully from: {model_path}\")\n",
    "        print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading RetinaNet model: {e}\")\n",
    "        print(\"Creating new model with random weights for testing...\")\n",
    "        \n",
    "        # Create a new model for testing\n",
    "        model = model.resnet50(num_classes=num_classes, pretrained=False)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        return model\n",
    "\n",
    "# Load RetinaNet model (uncomment when model is available)\n",
    "# retinanet_model = load_retinanet_model(RETINANET_CONFIG['MODEL_PATH'], RETINANET_CONFIG['NUM_CLASSES'], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetinaNet Dataset Loading for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retinanet_test_dataset(config, data_paths):\n",
    "    \"\"\"\n",
    "    Create test dataset for RetinaNet evaluation\n",
    "    \"\"\"\n",
    "    if config['DATASET_TYPE'] == 'csv':\n",
    "        dataset_test = CSVDataset(\n",
    "            train_file=data_paths['csv_test'],\n",
    "            class_list=data_paths['csv_classes'],\n",
    "            transform=transforms.Compose([Normalizer(), Resizer()])\n",
    "        )\n",
    "    elif config['DATASET_TYPE'] == 'coco':\n",
    "        dataset_test = CocoDataset(\n",
    "            data_paths['coco_path'],\n",
    "            set_name='test2017',\n",
    "            transform=transforms.Compose([Normalizer(), Resizer()])\n",
    "        )\n",
    "    \n",
    "    # Create data loader\n",
    "    sampler_test = AspectRatioBasedSampler(dataset_test, batch_size=config['BATCH_SIZE'], drop_last=False)\n",
    "    dataloader_test = DataLoader(dataset_test, num_workers=0, collate_fn=collater, batch_sampler=sampler_test)\n",
    "    \n",
    "    return dataloader_test, dataset_test\n",
    "\n",
    "# Create test dataset (uncomment when dataset is available)\n",
    "# test_dataloader, test_dataset = create_retinanet_test_dataset(RETINANET_CONFIG, RETINANET_DATA_PATHS)\n",
    "# print(f'Test dataset size: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetinaNet Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_retinanet_single_image(model, image, device, config):\n",
    "    \"\"\"\n",
    "    Perform inference on a single image\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Prepare image\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = torch.from_numpy(image).float()\n",
    "        \n",
    "        if len(image.shape) == 3:\n",
    "            image = image.unsqueeze(0)\n",
    "        \n",
    "        image = image.to(device)\n",
    "        \n",
    "        # Inference\n",
    "        start_time = time.time()\n",
    "        scores, classification, transformed_anchors = model(image)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Process predictions\n",
    "        scores = scores.cpu().numpy()\n",
    "        classification = classification.cpu().numpy()\n",
    "        transformed_anchors = transformed_anchors.cpu().numpy()\n",
    "        \n",
    "        # Filter by confidence threshold\n",
    "        indices = np.where(scores > config['CONFIDENCE_THRESHOLD'])[0]\n",
    "        \n",
    "        if len(indices) == 0:\n",
    "            return [], [], [], inference_time\n",
    "        \n",
    "        # Get filtered predictions\n",
    "        filtered_scores = scores[indices]\n",
    "        filtered_classes = classification[indices]\n",
    "        filtered_boxes = transformed_anchors[indices]\n",
    "        \n",
    "        # Apply NMS\n",
    "        keep_indices = torch.ops.torchvision.nms(\n",
    "            torch.from_numpy(filtered_boxes),\n",
    "            torch.from_numpy(filtered_scores),\n",
    "            config['NMS_THRESHOLD']\n",
    "        )\n",
    "        \n",
    "        final_scores = filtered_scores[keep_indices]\n",
    "        final_classes = filtered_classes[keep_indices]\n",
    "        final_boxes = filtered_boxes[keep_indices]\n",
    "        \n",
    "        return final_boxes, final_scores, final_classes, inference_time\n",
    "\n",
    "def batch_inference_retinanet(model, dataloader, device, config):\n",
    "    \"\"\"\n",
    "    Perform batch inference on test dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_ground_truths = []\n",
    "    inference_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(tqdm(dataloader, desc=\"Running inference\")):\n",
    "            images = data['img'].to(device)\n",
    "            annotations = data['annot']\n",
    "            \n",
    "            # Inference\n",
    "            start_time = time.time()\n",
    "            scores, classification, transformed_anchors = model(images)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Process batch predictions\n",
    "            batch_predictions = []\n",
    "            \n",
    "            for i in range(images.shape[0]):\n",
    "                # Get predictions for this image\n",
    "                img_scores = scores[i].cpu().numpy()\n",
    "                img_classes = classification[i].cpu().numpy()\n",
    "                img_boxes = transformed_anchors[i].cpu().numpy()\n",
    "                \n",
    "                # Filter by confidence\n",
    "                indices = np.where(img_scores > config['CONFIDENCE_THRESHOLD'])[0]\n",
    "                \n",
    "                if len(indices) > 0:\n",
    "                    img_predictions = {\n",
    "                        'boxes': img_boxes[indices],\n",
    "                        'scores': img_scores[indices],\n",
    "                        'classes': img_classes[indices]\n",
    "                    }\n",
    "                else:\n",
    "                    img_predictions = {\n",
    "                        'boxes': np.array([]),\n",
    "                        'scores': np.array([]),\n",
    "                        'classes': np.array([])\n",
    "                    }\n",
    "                \n",
    "                batch_predictions.append(img_predictions)\n",
    "            \n",
    "            all_predictions.extend(batch_predictions)\n",
    "            all_ground_truths.extend(annotations)\n",
    "            inference_times.append(inference_time)\n",
    "    \n",
    "    return all_predictions, all_ground_truths, inference_times\n",
    "\n",
    "# Example usage (uncomment when model and data are ready)\n",
    "# predictions, ground_truths, times = batch_inference_retinanet(retinanet_model, test_dataloader, device, RETINANET_CONFIG)\n",
    "# print(f'Average inference time per batch: {np.mean(times):.4f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetinaNet Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU between two bounding boxes\n",
    "    \"\"\"\n",
    "    # Box format: [x1, y1, x2, y2]\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x2 - x1) * (y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def evaluate_retinanet_predictions(predictions, ground_truths, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate RetinaNet predictions using mAP metrics\n",
    "    \"\"\"\n",
    "    all_detections = []\n",
    "    all_annotations = []\n",
    "    \n",
    "    for i, (pred, gt) in enumerate(zip(predictions, ground_truths)):\n",
    "        # Process predictions\n",
    "        if len(pred['boxes']) > 0:\n",
    "            for j, (box, score, cls) in enumerate(zip(pred['boxes'], pred['scores'], pred['classes'])):\n",
    "                all_detections.append({\n",
    "                    'image_id': i,\n",
    "                    'bbox': box,\n",
    "                    'score': score,\n",
    "                    'class': int(cls)\n",
    "                })\n",
    "        \n",
    "        # Process ground truth\n",
    "        if gt is not None and len(gt) > 0:\n",
    "            for annotation in gt:\n",
    "                if annotation[4] != -1:  # Valid annotation\n",
    "                    all_annotations.append({\n",
    "                        'image_id': i,\n",
    "                        'bbox': annotation[:4],\n",
    "                        'class': int(annotation[4])\n",
    "                    })\n",
    "    \n",
    "    # Calculate mAP\n",
    "    if len(all_detections) == 0 or len(all_annotations) == 0:\n",
    "        return {\n",
    "            'mAP': 0.0,\n",
    "            'mAP_50': 0.0,\n",
    "            'mAP_75': 0.0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1_score': 0.0\n",
    "        }\n",
    "    \n",
    "    # Group by class\n",
    "    detections_by_class = defaultdict(list)\n",
    "    annotations_by_class = defaultdict(list)\n",
    "    \n",
    "    for det in all_detections:\n",
    "        detections_by_class[det['class']].append(det)\n",
    "    \n",
    "    for ann in all_annotations:\n",
    "        annotations_by_class[ann['class']].append(ann)\n",
    "    \n",
    "    # Calculate AP for each class\n",
    "    aps = []\n",
    "    \n",
    "    for class_id in annotations_by_class.keys():\n",
    "        class_detections = detections_by_class.get(class_id, [])\n",
    "        class_annotations = annotations_by_class[class_id]\n",
    "        \n",
    "        # Sort detections by score\n",
    "        class_detections = sorted(class_detections, key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        tp = np.zeros(len(class_detections))\n",
    "        fp = np.zeros(len(class_detections))\n",
    "        \n",
    "        # Track which annotations have been matched\n",
    "        matched_annotations = set()\n",
    "        \n",
    "        for i, detection in enumerate(class_detections):\n",
    "            # Find best matching annotation\n",
    "            best_iou = 0.0\n",
    "            best_ann_idx = -1\n",
    "            \n",
    "            for j, annotation in enumerate(class_annotations):\n",
    "                if annotation['image_id'] == detection['image_id']:\n",
    "                    iou = calculate_iou(detection['bbox'], annotation['bbox'])\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_ann_idx = j\n",
    "            \n",
    "            if best_iou >= iou_threshold and best_ann_idx not in matched_annotations:\n",
    "                tp[i] = 1\n",
    "                matched_annotations.add(best_ann_idx)\n",
    "            else:\n",
    "                fp[i] = 1\n",
    "        \n",
    "        # Calculate precision and recall curves\n",
    "        tp_cumsum = np.cumsum(tp)\n",
    "        fp_cumsum = np.cumsum(fp)\n",
    "        \n",
    "        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-8)\n",
    "        recalls = tp_cumsum / len(class_annotations)\n",
    "        \n",
    "        # Calculate AP using 11-point interpolation\n",
    "        ap = 0.0\n",
    "        for t in np.arange(0, 1.1, 0.1):\n",
    "            if np.sum(recalls >= t) == 0:\n",
    "                p = 0\n",
    "            else:\n",
    "                p = np.max(precisions[recalls >= t])\n",
    "            ap += p / 11.0\n",
    "        \n",
    "        aps.append(ap)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    mAP = np.mean(aps) if aps else 0.0\n",
    "    \n",
    "    # Calculate precision, recall, F1 at IoU threshold\n",
    "    total_tp = sum(tp.sum() for tp in [np.array([1 if calculate_iou(det['bbox'], ann['bbox']) >= iou_threshold else 0 \n",
    "                                                for ann in all_annotations if ann['image_id'] == det['image_id'] and ann['class'] == det['class']])\n",
    "                                      for det in all_detections])\n",
    "    \n",
    "    precision = total_tp / len(all_detections) if all_detections else 0.0\n",
    "    recall = total_tp / len(all_annotations) if all_annotations else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'mAP': mAP,\n",
    "        'mAP_50': mAP,  # This is mAP at IoU=0.5\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'num_detections': len(all_detections),\n",
    "        'num_annotations': len(all_annotations)\n",
    "    }\n",
    "\n",
    "# Example evaluation (uncomment when predictions are available)\n",
    "# retinanet_metrics = evaluate_retinanet_predictions(predictions, ground_truths)\n",
    "# print(\"RetinaNet Evaluation Results:\")\n",
    "# for key, value in retinanet_metrics.items():\n",
    "#     print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. U-Net Inference and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Model Loading and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net imports\n",
    "from Unet.loss import dice_loss, dice\n",
    "from Unet.Unet import UNet\n",
    "from Unet.preprocessing import *\n",
    "from Unet.datagenerater import Dental_Single_Data_Generator\n",
    "from Unet.utils import *\n",
    "\n",
    "# U-Net Inference Configuration\n",
    "UNET_CONFIG = {\n",
    "    'MODEL_PATH': './unet_weights/',  # Directory containing landmark models\n",
    "    'MULTICLASS_MODEL_PATH': './unet_weights/best_multiclass_unet.pth',\n",
    "    'IMAGE_SIZE': (512, 512),\n",
    "    'NUM_LANDMARKS': 14,\n",
    "    'BATCH_SIZE': 1,\n",
    "    'CONFIDENCE_THRESHOLD': 0.5,\n",
    "    'USE_MULTICLASS': False,  # Set to True for single multi-class model\n",
    "    'ENCODER_NAME': 'vgg16',\n",
    "    'DISTANCE_THRESHOLD': 10.0  # Pixel distance threshold for landmark accuracy\n",
    "}\n",
    "\n",
    "# Dataset paths for evaluation\n",
    "UNET_DATA_PATHS = {\n",
    "    'image_path': './dataset/images/',\n",
    "    'label_path': './dataset/labels/',\n",
    "    'test_split': 0.2  # 20% for testing\n",
    "}\n",
    "\n",
    "print(\"U-Net inference configuration loaded.\")\n",
    "print(f\"Number of landmarks: {UNET_CONFIG['NUM_LANDMARKS']}\")\n",
    "print(f\"Multi-class model: {UNET_CONFIG['USE_MULTICLASS']}\")\n",
    "print(f\"Distance threshold: {UNET_CONFIG['DISTANCE_THRESHOLD']} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unet_models(config, device):\n",
    "    \"\"\"\n",
    "    Load U-Net models for landmark detection\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    if config['USE_MULTICLASS']:\n",
    "        # Load single multi-class model\n",
    "        try:\n",
    "            import segmentation_models_pytorch as smp\n",
    "            model = smp.Unet(\n",
    "                encoder_name=config['ENCODER_NAME'],\n",
    "                decoder_attention_type='scse',\n",
    "                in_channels=1,\n",
    "                classes=config['NUM_LANDMARKS']\n",
    "            )\n",
    "        except ImportError:\n",
    "            model = UNet(n_channels=1, n_classes=config['NUM_LANDMARKS'])\n",
    "        \n",
    "        # Load weights\n",
    "        try:\n",
    "            state_dict = torch.load(config['MULTICLASS_MODEL_PATH'], map_location=device)\n",
    "            model.load_state_dict(state_dict)\n",
    "            print(f\"Multi-class U-Net model loaded from: {config['MULTICLASS_MODEL_PATH']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading multi-class model: {e}\")\n",
    "            print(\"Using model with random weights...\")\n",
    "        \n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        models.append(model)\n",
    "        \n",
    "    else:\n",
    "        # Load separate models for each landmark\n",
    "        for landmark_idx in range(config['NUM_LANDMARKS']):\n",
    "            try:\n",
    "                import segmentation_models_pytorch as smp\n",
    "                model = smp.Unet(\n",
    "                    encoder_name=config['ENCODER_NAME'],\n",
    "                    decoder_attention_type='scse',\n",
    "                    in_channels=1,\n",
    "                    classes=1\n",
    "                )\n",
    "            except ImportError:\n",
    "                model = UNet(n_channels=1, n_classes=1)\n",
    "            \n",
    "            # Load weights for this landmark\n",
    "            model_path = os.path.join(config['MODEL_PATH'], str(landmark_idx), 'weight.pth')\n",
    "            try:\n",
    "                state_dict = torch.load(model_path, map_location=device)\n",
    "                model.load_state_dict(state_dict)\n",
    "                print(f\"Landmark {landmark_idx} model loaded from: {model_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model for landmark {landmark_idx}: {e}\")\n",
    "                print(\"Using model with random weights...\")\n",
    "            \n",
    "            model.eval()\n",
    "            model.to(device)\n",
    "            models.append(model)\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Load U-Net models (uncomment when models are available)\n",
    "# unet_models = load_unet_models(UNET_CONFIG, device)\n",
    "# print(f\"Loaded {len(unet_models)} U-Net model(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Dataset Loading for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_unet_test_dataset(data_paths, config):\n",
    "    \"\"\"\n",
    "    Prepare U-Net test dataset\n",
    "    \"\"\"\n",
    "    # Get all image and label files\n",
    "    image_files = natsorted(glob.glob(os.path.join(data_paths['image_path'], '*.png')))\n",
    "    label_files = natsorted(glob.glob(os.path.join(data_paths['label_path'], '*.npy')))\n",
    "    \n",
    "    # Match image and label files\n",
    "    matched_pairs = []\n",
    "    for label_file in label_files:\n",
    "        base_name = os.path.basename(label_file).split('.')[0]\n",
    "        matching_images = [img for img in image_files if base_name in os.path.basename(img)]\n",
    "        if matching_images:\n",
    "            matched_pairs.append((matching_images[0], label_file))\n",
    "    \n",
    "    # Split into train/test\n",
    "    split_idx = int(len(matched_pairs) * (1 - data_paths['test_split']))\n",
    "    \n",
    "    test_pairs = matched_pairs[split_idx:]\n",
    "    \n",
    "    x_test = [pair[0] for pair in test_pairs]\n",
    "    y_test = [pair[1] for pair in test_pairs]\n",
    "    \n",
    "    print(f'Test samples: {len(x_test)}')\n",
    "    \n",
    "    return x_test, y_test\n",
    "\n",
    "def create_unet_test_dataloaders(x_test, y_test, config):\n",
    "    \"\"\"\n",
    "    Create test data loaders for U-Net evaluation\n",
    "    \"\"\"\n",
    "    transform_test = transforms.Compose([\n",
    "        ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    if config['USE_MULTICLASS']:\n",
    "        # Single multi-class dataset\n",
    "        testset = Dental_Single_Data_Generator(\n",
    "            config['IMAGE_SIZE'], x_test, y_test, \n",
    "            landmark_num=-1, mode=\"test\", transform=transform_test\n",
    "        )\n",
    "        \n",
    "        testloader = DataLoader(testset, batch_size=config['BATCH_SIZE'], shuffle=False)\n",
    "        return testloader\n",
    "    else:\n",
    "        # Separate datasets for each landmark\n",
    "        test_loaders = []\n",
    "        \n",
    "        for landmark_idx in range(config['NUM_LANDMARKS']):\n",
    "            testset = Dental_Single_Data_Generator(\n",
    "                config['IMAGE_SIZE'], x_test, y_test, \n",
    "                landmark_num=landmark_idx, mode=\"test\", transform=transform_test\n",
    "            )\n",
    "            \n",
    "            testloader = DataLoader(testset, batch_size=config['BATCH_SIZE'], shuffle=False)\n",
    "            test_loaders.append(testloader)\n",
    "        \n",
    "        return test_loaders\n",
    "\n",
    "# Prepare test dataset (uncomment when dataset is available)\n",
    "# x_test, y_test = prepare_unet_test_dataset(UNET_DATA_PATHS, UNET_CONFIG)\n",
    "# test_loaders = create_unet_test_dataloaders(x_test, y_test, UNET_CONFIG)\n",
    "# print(f\"Created {len(test_loaders) if isinstance(test_loaders, list) else 1} test loader(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmark_coordinates(mask, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Extract landmark coordinates from segmentation mask\n",
    "    \"\"\"\n",
    "    # Apply threshold\n",
    "    binary_mask = (mask > threshold).astype(np.uint8)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Get the largest contour\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # Get centroid\n",
    "    M = cv2.moments(largest_contour)\n",
    "    if M[\"m00\"] == 0:\n",
    "        return None\n",
    "    \n",
    "    cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "    cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "    \n",
    "    return (cx, cy)\n",
    "\n",
    "def inference_unet_single_image(models, image, device, config):\n",
    "    \"\"\"\n",
    "    Perform U-Net inference on a single image\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    inference_times = []\n",
    "    \n",
    "    # Prepare image\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = torch.from_numpy(image).float()\n",
    "    \n",
    "    if len(image.shape) == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    \n",
    "    image = image.to(device)\n",
    "    \n",
    "    if config['USE_MULTICLASS']:\n",
    "        # Single multi-class model\n",
    "        model = models[0]\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            output = model(image)\n",
    "            output = torch.sigmoid(output)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Process each landmark channel\n",
    "            output_np = output.cpu().numpy().squeeze()\n",
    "            \n",
    "            for landmark_idx in range(config['NUM_LANDMARKS']):\n",
    "                landmark_mask = output_np[landmark_idx]\n",
    "                coords = extract_landmark_coordinates(landmark_mask, config['CONFIDENCE_THRESHOLD'])\n",
    "                predictions.append(coords)\n",
    "            \n",
    "            inference_times.append(inference_time)\n",
    "    else:\n",
    "        # Separate models for each landmark\n",
    "        for landmark_idx, model in enumerate(models):\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                start_time = time.time()\n",
    "                output = model(image)\n",
    "                output = torch.sigmoid(output)\n",
    "                inference_time = time.time() - start_time\n",
    "                \n",
    "                # Extract coordinates\n",
    "                mask = output.cpu().numpy().squeeze()\n",
    "                coords = extract_landmark_coordinates(mask, config['CONFIDENCE_THRESHOLD'])\n",
    "                predictions.append(coords)\n",
    "                inference_times.append(inference_time)\n",
    "    \n",
    "    return predictions, inference_times\n",
    "\n",
    "def batch_inference_unet(models, test_loaders, device, config):\n",
    "    \"\"\"\n",
    "    Perform batch inference on U-Net test dataset\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    all_ground_truths = []\n",
    "    all_inference_times = []\n",
    "    \n",
    "    if config['USE_MULTICLASS']:\n",
    "        # Multi-class model inference\n",
    "        model = models[0]\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, sample in enumerate(tqdm(test_loaders, desc=\"Running U-Net inference\")):\n",
    "                images = sample['image'].to(device)\n",
    "                landmarks = sample['landmarks'].cpu().numpy()\n",
    "                \n",
    "                start_time = time.time()\n",
    "                outputs = model(images)\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                inference_time = time.time() - start_time\n",
    "                \n",
    "                # Process batch\n",
    "                batch_predictions = []\n",
    "                batch_ground_truths = []\n",
    "                \n",
    "                for i in range(images.shape[0]):\n",
    "                    # Extract predictions for each landmark\n",
    "                    img_predictions = []\n",
    "                    output_np = outputs[i].cpu().numpy()\n",
    "                    \n",
    "                    for landmark_idx in range(config['NUM_LANDMARKS']):\n",
    "                        landmark_mask = output_np[landmark_idx]\n",
    "                        coords = extract_landmark_coordinates(landmark_mask, config['CONFIDENCE_THRESHOLD'])\n",
    "                        img_predictions.append(coords)\n",
    "                    \n",
    "                    batch_predictions.append(img_predictions)\n",
    "                    \n",
    "                    # Extract ground truth coordinates\n",
    "                    gt_coords = []\n",
    "                    for landmark_idx in range(config['NUM_LANDMARKS']):\n",
    "                        gt_mask = landmarks[i, landmark_idx]\n",
    "                        gt_coord = extract_landmark_coordinates(gt_mask, 0.5)\n",
    "                        gt_coords.append(gt_coord)\n",
    "                    \n",
    "                    batch_ground_truths.append(gt_coords)\n",
    "                \n",
    "                all_predictions.extend(batch_predictions)\n",
    "                all_ground_truths.extend(batch_ground_truths)\n",
    "                all_inference_times.append(inference_time)\n",
    "    else:\n",
    "        # Separate models inference\n",
    "        num_samples = len(test_loaders[0].dataset)\n",
    "        \n",
    "        # Initialize prediction and ground truth arrays\n",
    "        predictions_by_sample = [[] for _ in range(num_samples)]\n",
    "        ground_truths_by_sample = [[] for _ in range(num_samples)]\n",
    "        \n",
    "        # Process each landmark model\n",
    "        for landmark_idx, (model, test_loader) in enumerate(zip(models, test_loaders)):\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_idx, sample in enumerate(tqdm(test_loader, desc=f\"Landmark {landmark_idx + 1}/{config['NUM_LANDMARKS']}\")):\n",
    "                    images = sample['image'].to(device)\n",
    "                    landmarks = sample['landmarks'].cpu().numpy()\n",
    "                    \n",
    "                    start_time = time.time()\n",
    "                    outputs = model(images)\n",
    "                    outputs = torch.sigmoid(outputs)\n",
    "                    inference_time = time.time() - start_time\n",
    "                    \n",
    "                    # Process batch\n",
    "                    for i in range(images.shape[0]):\n",
    "                        sample_idx = batch_idx * config['BATCH_SIZE'] + i\n",
    "                        \n",
    "                        # Extract prediction coordinates\n",
    "                        mask = outputs[i].cpu().numpy().squeeze()\n",
    "                        coords = extract_landmark_coordinates(mask, config['CONFIDENCE_THRESHOLD'])\n",
    "                        \n",
    "                        # Store prediction\n",
    "                        if len(predictions_by_sample[sample_idx]) <= landmark_idx:\n",
    "                            predictions_by_sample[sample_idx].extend([None] * (landmark_idx + 1 - len(predictions_by_sample[sample_idx])))\n",
    "                        predictions_by_sample[sample_idx][landmark_idx] = coords\n",
    "                        \n",
    "                        # Extract ground truth coordinates\n",
    "                        gt_mask = landmarks[i].squeeze()\n",
    "                        gt_coord = extract_landmark_coordinates(gt_mask, 0.5)\n",
    "                        \n",
    "                        # Store ground truth\n",
    "                        if len(ground_truths_by_sample[sample_idx]) <= landmark_idx:\n",
    "                            ground_truths_by_sample[sample_idx].extend([None] * (landmark_idx + 1 - len(ground_truths_by_sample[sample_idx])))\n",
    "                        ground_truths_by_sample[sample_idx][landmark_idx] = gt_coord\n",
    "                    \n",
    "                    all_inference_times.append(inference_time)\n",
    "        \n",
    "        all_predictions = predictions_by_sample\n",
    "        all_ground_truths = ground_truths_by_sample\n",
    "    \n",
    "    return all_predictions, all_ground_truths, all_inference_times\n",
    "\n",
    "# Example usage (uncomment when models and data are ready)\n",
    "# unet_predictions, unet_ground_truths, unet_times = batch_inference_unet(unet_models, test_loaders, device, UNET_CONFIG)\n",
    "# print(f'U-Net inference completed on {len(unet_predictions)} samples')\n",
    "# print(f'Average inference time: {np.mean(unet_times):.4f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_euclidean_distance(coord1, coord2):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance between two coordinates\n",
    "    \"\"\"\n",
    "    if coord1 is None or coord2 is None:\n",
    "        return float('inf')\n",
    "    \n",
    "    return np.sqrt((coord1[0] - coord2[0])**2 + (coord1[1] - coord2[1])**2)\n",
    "\n",
    "def evaluate_unet_predictions(predictions, ground_truths, config):\n",
    "    \"\"\"\n",
    "    Evaluate U-Net predictions using landmark detection metrics\n",
    "    \"\"\"\n",
    "    num_landmarks = config['NUM_LANDMARKS']\n",
    "    distance_threshold = config['DISTANCE_THRESHOLD']\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    landmark_distances = [[] for _ in range(num_landmarks)]\n",
    "    landmark_accuracies = [[] for _ in range(num_landmarks)]\n",
    "    detection_rates = [0] * num_landmarks\n",
    "    \n",
    "    total_samples = len(predictions)\n",
    "    \n",
    "    # Process each sample\n",
    "    for sample_idx, (pred_landmarks, gt_landmarks) in enumerate(zip(predictions, ground_truths)):\n",
    "        for landmark_idx in range(num_landmarks):\n",
    "            pred_coord = pred_landmarks[landmark_idx] if landmark_idx < len(pred_landmarks) else None\n",
    "            gt_coord = gt_landmarks[landmark_idx] if landmark_idx < len(gt_landmarks) else None\n",
    "            \n",
    "            if gt_coord is not None:\n",
    "                if pred_coord is not None:\n",
    "                    # Calculate distance\n",
    "                    distance = calculate_euclidean_distance(pred_coord, gt_coord)\n",
    "                    landmark_distances[landmark_idx].append(distance)\n",
    "                    \n",
    "                    # Check if within threshold\n",
    "                    is_accurate = distance <= distance_threshold\n",
    "                    landmark_accuracies[landmark_idx].append(is_accurate)\n",
    "                    \n",
    "                    # Update detection rate\n",
    "                    if is_accurate:\n",
    "                        detection_rates[landmark_idx] += 1\n",
    "                else:\n",
    "                    # No prediction made\n",
    "                    landmark_distances[landmark_idx].append(float('inf'))\n",
    "                    landmark_accuracies[landmark_idx].append(False)\n",
    "    \n",
    "    # Calculate metrics for each landmark\n",
    "    landmark_metrics = []\n",
    "    \n",
    "    for landmark_idx in range(num_landmarks):\n",
    "        distances = landmark_distances[landmark_idx]\n",
    "        accuracies = landmark_accuracies[landmark_idx]\n",
    "        \n",
    "        if len(distances) > 0:\n",
    "            # Filter out infinite distances for mean calculation\n",
    "            finite_distances = [d for d in distances if d != float('inf')]\n",
    "            \n",
    "            metrics = {\n",
    "                'landmark_id': landmark_idx,\n",
    "                'mean_distance': np.mean(finite_distances) if finite_distances else float('inf'),\n",
    "                'median_distance': np.median(finite_distances) if finite_distances else float('inf'),\n",
    "                'std_distance': np.std(finite_distances) if finite_distances else 0.0,\n",
    "                'accuracy': np.mean(accuracies) if accuracies else 0.0,\n",
    "                'detection_rate': detection_rates[landmark_idx] / len(accuracies) if accuracies else 0.0,\n",
    "                'num_samples': len(accuracies)\n",
    "            }\n",
    "        else:\n",
    "            metrics = {\n",
    "                'landmark_id': landmark_idx,\n",
    "                'mean_distance': float('inf'),\n",
    "                'median_distance': float('inf'),\n",
    "                'std_distance': 0.0,\n",
    "                'accuracy': 0.0,\n",
    "                'detection_rate': 0.0,\n",
    "                'num_samples': 0\n",
    "            }\n",
    "        \n",
    "        landmark_metrics.append(metrics)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    all_distances = [d for distances in landmark_distances for d in distances if d != float('inf')]\n",
    "    all_accuracies = [acc for accuracies in landmark_accuracies for acc in accuracies]\n",
    "    \n",
    "    overall_metrics = {\n",
    "        'overall_mean_distance': np.mean(all_distances) if all_distances else float('inf'),\n",
    "        'overall_median_distance': np.median(all_distances) if all_distances else float('inf'),\n",
    "        'overall_std_distance': np.std(all_distances) if all_distances else 0.0,\n",
    "        'overall_accuracy': np.mean(all_accuracies) if all_accuracies else 0.0,\n",
    "        'overall_detection_rate': sum(detection_rates) / (total_samples * num_landmarks),\n",
    "        'total_samples': total_samples,\n",
    "        'total_landmarks': num_landmarks\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'landmark_metrics': landmark_metrics,\n",
    "        'overall_metrics': overall_metrics\n",
    "    }\n",
    "\n",
    "def calculate_dice_score_batch(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Calculate Dice score for segmentation masks\n",
    "    \"\"\"\n",
    "    dice_scores = []\n",
    "    \n",
    "    for pred_landmarks, gt_landmarks in zip(predictions, ground_truths):\n",
    "        sample_dice_scores = []\n",
    "        \n",
    "        for pred_coord, gt_coord in zip(pred_landmarks, gt_landmarks):\n",
    "            if pred_coord is not None and gt_coord is not None:\n",
    "                # Create binary masks for Dice calculation\n",
    "                # This is a simplified version - in practice, you'd use the actual segmentation masks\n",
    "                pred_mask = np.zeros((512, 512))  # Adjust size as needed\n",
    "                gt_mask = np.zeros((512, 512))\n",
    "                \n",
    "                # Create small circles around the coordinates\n",
    "                cv2.circle(pred_mask, pred_coord, 5, 1, -1)\n",
    "                cv2.circle(gt_mask, gt_coord, 5, 1, -1)\n",
    "                \n",
    "                # Calculate Dice score\n",
    "                intersection = np.sum(pred_mask * gt_mask)\n",
    "                union = np.sum(pred_mask) + np.sum(gt_mask)\n",
    "                \n",
    "                dice_score = 2 * intersection / union if union > 0 else 0.0\n",
    "                sample_dice_scores.append(dice_score)\n",
    "            else:\n",
    "                sample_dice_scores.append(0.0)\n",
    "        \n",
    "        dice_scores.append(np.mean(sample_dice_scores))\n",
    "    \n",
    "    return np.mean(dice_scores)\n",
    "\n",
    "# Example evaluation (uncomment when predictions are available)\n",
    "# unet_metrics = evaluate_unet_predictions(unet_predictions, unet_ground_truths, UNET_CONFIG)\n",
    "\n",
    "# print(\"\\nU-Net Evaluation Results:\")\n",
    "# print(\"=\" * 50)\n",
    "# print(f\"Overall Accuracy: {unet_metrics['overall_metrics']['overall_accuracy']:.4f}\")\n",
    "# print(f\"Overall Mean Distance: {unet_metrics['overall_metrics']['overall_mean_distance']:.2f} pixels\")\n",
    "# print(f\"Overall Detection Rate: {unet_metrics['overall_metrics']['overall_detection_rate']:.4f}\")\n",
    "\n",
    "# print(\"\\nPer-Landmark Results:\")\n",
    "# for landmark_metric in unet_metrics['landmark_metrics']:\n",
    "#     print(f\"Landmark {landmark_metric['landmark_id'] + 1:2d}: \"\n",
    "#           f\"Acc={landmark_metric['accuracy']:.3f}, \"\n",
    "#           f\"Dist={landmark_metric['mean_distance']:.2f}px, \"\n",
    "#           f\"Det={landmark_metric['detection_rate']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_retinanet_predictions(images, predictions, ground_truths, class_names=None, num_samples=4):\n",
    "    \"\"\"\n",
    "    Visualize RetinaNet predictions\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(num_samples * 5, 10))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for i in range(min(num_samples, len(images))):\n",
    "        image = images[i]\n",
    "        pred = predictions[i]\n",
    "        gt = ground_truths[i]\n",
    "        \n",
    "        # Original image with ground truth\n",
    "        axes[0, i].imshow(image, cmap='gray')\n",
    "        axes[0, i].set_title(f'Ground Truth - Sample {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Draw ground truth boxes\n",
    "        if gt is not None and len(gt) > 0:\n",
    "            for annotation in gt:\n",
    "                if annotation[4] != -1:  # Valid annotation\n",
    "                    box = annotation[:4]\n",
    "                    rect = patches.Rectangle(\n",
    "                        (box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "                        linewidth=2, edgecolor='green', facecolor='none'\n",
    "                    )\n",
    "                    axes[0, i].add_patch(rect)\n",
    "        \n",
    "        # Image with predictions\n",
    "        axes[1, i].imshow(image, cmap='gray')\n",
    "        axes[1, i].set_title(f'Predictions - Sample {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Draw prediction boxes\n",
    "        if len(pred['boxes']) > 0:\n",
    "            for box, score, cls in zip(pred['boxes'], pred['scores'], pred['classes']):\n",
    "                rect = patches.Rectangle(\n",
    "                    (box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "                    linewidth=2, edgecolor='red', facecolor='none'\n",
    "                )\n",
    "                axes[1, i].add_patch(rect)\n",
    "                \n",
    "                # Add score text\n",
    "                class_name = class_names[int(cls)] if class_names else f'Class {int(cls)}'\n",
    "                axes[1, i].text(\n",
    "                    box[0], box[1] - 5, f'{class_name}: {score:.2f}',\n",
    "                    color='red', fontsize=8, weight='bold'\n",
    "                )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_unet_predictions(images, predictions, ground_truths, config, num_samples=4):\n",
    "    \"\"\"\n",
    "    Visualize U-Net landmark predictions\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=(num_samples * 5, 15))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(3, 1)\n",
    "    \n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, config['NUM_LANDMARKS']))\n",
    "    \n",
    "    for i in range(min(num_samples, len(images))):\n",
    "        image = images[i]\n",
    "        pred_landmarks = predictions[i]\n",
    "        gt_landmarks = ground_truths[i]\n",
    "        \n",
    "        # Original image\n",
    "        axes[0, i].imshow(image, cmap='gray')\n",
    "        axes[0, i].set_title(f'Original Image - Sample {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Image with ground truth landmarks\n",
    "        axes[1, i].imshow(image, cmap='gray')\n",
    "        axes[1, i].set_title(f'Ground Truth Landmarks - Sample {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        for landmark_idx, gt_coord in enumerate(gt_landmarks):\n",
    "            if gt_coord is not None:\n",
    "                axes[1, i].scatter(\n",
    "                    gt_coord[0], gt_coord[1],\n",
    "                    c=[colors[landmark_idx]], s=50, marker='o',\n",
    "                    label=f'L{landmark_idx+1}' if i == 0 else None\n",
    "                )\n",
    "        \n",
    "        # Image with predicted landmarks\n",
    "        axes[2, i].imshow(image, cmap='gray')\n",
    "        axes[2, i].set_title(f'Predicted Landmarks - Sample {i+1}')\n",
    "        axes[2, i].axis('off')\n",
    "        \n",
    "        for landmark_idx, pred_coord in enumerate(pred_landmarks):\n",
    "            if pred_coord is not None:\n",
    "                axes[2, i].scatter(\n",
    "                    pred_coord[0], pred_coord[1],\n",
    "                    c=[colors[landmark_idx]], s=50, marker='x',\n",
    "                    label=f'L{landmark_idx+1}' if i == 0 else None\n",
    "                )\n",
    "        \n",
    "        # Draw lines between GT and predictions\n",
    "        for landmark_idx, (gt_coord, pred_coord) in enumerate(zip(gt_landmarks, pred_landmarks)):\n",
    "            if gt_coord is not None and pred_coord is not None:\n",
    "                distance = calculate_euclidean_distance(gt_coord, pred_coord)\n",
    "                axes[2, i].plot(\n",
    "                    [gt_coord[0], pred_coord[0]],\n",
    "                    [gt_coord[1], pred_coord[1]],\n",
    "                    'r--', alpha=0.5, linewidth=1\n",
    "                )\n",
    "                \n",
    "                # Add distance text\n",
    "                mid_x = (gt_coord[0] + pred_coord[0]) / 2\n",
    "                mid_y = (gt_coord[1] + pred_coord[1]) / 2\n",
    "                axes[2, i].text(\n",
    "                    mid_x, mid_y, f'{distance:.1f}px',\n",
    "                    fontsize=8, color='red', weight='bold'\n",
    "                )\n",
    "    \n",
    "    # Add legend\n",
    "    if num_samples > 0:\n",
    "        axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        axes[2, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_evaluation_metrics(retinanet_metrics, unet_metrics, config):\n",
    "    \"\"\"\n",
    "    Plot evaluation metrics for both models\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # RetinaNet metrics\n",
    "    if retinanet_metrics:\n",
    "        retinanet_values = [retinanet_metrics['mAP'], retinanet_metrics['precision'], \n",
    "                           retinanet_metrics['recall'], retinanet_metrics['f1_score']]\n",
    "        retinanet_labels = ['mAP', 'Precision', 'Recall', 'F1-Score']\n",
    "        \n",
    "        axes[0, 0].bar(retinanet_labels, retinanet_values)\n",
    "        axes[0, 0].set_title('RetinaNet Metrics')\n",
    "        axes[0, 0].set_ylabel('Score')\n",
    "        axes[0, 0].set_ylim(0, 1)\n",
    "        \n",
    "        for i, v in enumerate(retinanet_values):\n",
    "            axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # U-Net overall metrics\n",
    "    if unet_metrics:\n",
    "        unet_values = [unet_metrics['overall_metrics']['overall_accuracy'], \n",
    "                      unet_metrics['overall_metrics']['overall_detection_rate']]\n",
    "        unet_labels = ['Accuracy', 'Detection Rate']\n",
    "        \n",
    "        axes[0, 1].bar(unet_labels, unet_values)\n",
    "        axes[0, 1].set_title('U-Net Overall Metrics')\n",
    "        axes[0, 1].set_ylabel('Score')\n",
    "        axes[0, 1].set_ylim(0, 1)\n",
    "        \n",
    "        for i, v in enumerate(unet_values):\n",
    "            axes[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # U-Net per-landmark accuracy\n",
    "    if unet_metrics:\n",
    "        landmark_accuracies = [m['accuracy'] for m in unet_metrics['landmark_metrics']]\n",
    "        landmark_ids = [f'L{i+1}' for i in range(len(landmark_accuracies))]\n",
    "        \n",
    "        axes[1, 0].bar(landmark_ids, landmark_accuracies)\n",
    "        axes[1, 0].set_title('U-Net Per-Landmark Accuracy')\n",
    "        axes[1, 0].set_ylabel('Accuracy')\n",
    "        axes[1, 0].set_ylim(0, 1)\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # U-Net per-landmark mean distance\n",
    "    if unet_metrics:\n",
    "        landmark_distances = [m['mean_distance'] if m['mean_distance'] != float('inf') else 0 \n",
    "                            for m in unet_metrics['landmark_metrics']]\n",
    "        \n",
    "        axes[1, 1].bar(landmark_ids, landmark_distances)\n",
    "        axes[1, 1].set_title('U-Net Per-Landmark Mean Distance')\n",
    "        axes[1, 1].set_ylabel('Distance (pixels)')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example visualization (uncomment when data is available)\n",
    "# visualize_retinanet_predictions(sample_images, predictions[:4], ground_truths[:4])\n",
    "# visualize_unet_predictions(sample_images, unet_predictions[:4], unet_ground_truths[:4], UNET_CONFIG)\n",
    "# plot_evaluation_metrics(retinanet_metrics, unet_metrics, UNET_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Analysis and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_report(retinanet_metrics, unet_metrics, retinanet_times, unet_times):\n",
    "    \"\"\"\n",
    "    Create comprehensive performance report\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'retinanet': {\n",
    "            'metrics': retinanet_metrics,\n",
    "            'performance': {\n",
    "                'mean_inference_time': np.mean(retinanet_times) if retinanet_times else 0,\n",
    "                'std_inference_time': np.std(retinanet_times) if retinanet_times else 0,\n",
    "                'fps': 1.0 / np.mean(retinanet_times) if retinanet_times and np.mean(retinanet_times) > 0 else 0\n",
    "            }\n",
    "        },\n",
    "        'unet': {\n",
    "            'metrics': unet_metrics,\n",
    "            'performance': {\n",
    "                'mean_inference_time': np.mean(unet_times) if unet_times else 0,\n",
    "                'std_inference_time': np.std(unet_times) if unet_times else 0,\n",
    "                'fps': 1.0 / np.mean(unet_times) if unet_times and np.mean(unet_times) > 0 else 0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "def save_results_to_csv(report, filename='inference_results.csv'):\n",
    "    \"\"\"\n",
    "    Save results to CSV file\n",
    "    \"\"\"\n",
    "    # Create RetinaNet results\n",
    "    retinanet_data = {\n",
    "        'Model': 'RetinaNet',\n",
    "        'mAP': report['retinanet']['metrics'].get('mAP', 0) if report['retinanet']['metrics'] else 0,\n",
    "        'Precision': report['retinanet']['metrics'].get('precision', 0) if report['retinanet']['metrics'] else 0,\n",
    "        'Recall': report['retinanet']['metrics'].get('recall', 0) if report['retinanet']['metrics'] else 0,\n",
    "        'F1-Score': report['retinanet']['metrics'].get('f1_score', 0) if report['retinanet']['metrics'] else 0,\n",
    "        'Inference Time (s)': report['retinanet']['performance']['mean_inference_time'],\n",
    "        'FPS': report['retinanet']['performance']['fps']\n",
    "    }\n",
    "    \n",
    "    # Create U-Net results\n",
    "    unet_data = {\n",
    "        'Model': 'U-Net',\n",
    "        'Accuracy': report['unet']['metrics']['overall_metrics']['overall_accuracy'] if report['unet']['metrics'] else 0,\n",
    "        'Detection Rate': report['unet']['metrics']['overall_metrics']['overall_detection_rate'] if report['unet']['metrics'] else 0,\n",
    "        'Mean Distance (px)': report['unet']['metrics']['overall_metrics']['overall_mean_distance'] if report['unet']['metrics'] else 0,\n",
    "        'Inference Time (s)': report['unet']['performance']['mean_inference_time'],\n",
    "        'FPS': report['unet']['performance']['fps']\n",
    "    }\n",
    "    \n",
    "    # Save to CSV\n",
    "    df_retinanet = pd.DataFrame([retinanet_data])\n",
    "    df_unet = pd.DataFrame([unet_data])\n",
    "    \n",
    "    df_retinanet.to_csv(f'retinanet_{filename}', index=False)\n",
    "    df_unet.to_csv(f'unet_{filename}', index=False)\n",
    "    \n",
    "    print(f\"Results saved to retinanet_{filename} and unet_{filename}\")\n",
    "\n",
    "def print_performance_summary(report):\n",
    "    \"\"\"\n",
    "    Print performance summary\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Report generated: {report['timestamp']}\")\n",
    "    \n",
    "    print(\"\\n🎯 RETINANET RESULTS:\")\n",
    "    print(\"-\" * 30)\n",
    "    if report['retinanet']['metrics']:\n",
    "        print(f\"mAP: {report['retinanet']['metrics'].get('mAP', 0):.4f}\")\n",
    "        print(f\"Precision: {report['retinanet']['metrics'].get('precision', 0):.4f}\")\n",
    "        print(f\"Recall: {report['retinanet']['metrics'].get('recall', 0):.4f}\")\n",
    "        print(f\"F1-Score: {report['retinanet']['metrics'].get('f1_score', 0):.4f}\")\n",
    "    else:\n",
    "        print(\"No RetinaNet metrics available\")\n",
    "    \n",
    "    print(f\"Inference Time: {report['retinanet']['performance']['mean_inference_time']:.4f} ± {report['retinanet']['performance']['std_inference_time']:.4f} seconds\")\n",
    "    print(f\"FPS: {report['retinanet']['performance']['fps']:.2f}\")\n",
    "    \n",
    "    print(\"\\n🎯 U-NET RESULTS:\")\n",
    "    print(\"-\" * 30)\n",
    "    if report['unet']['metrics']:\n",
    "        print(f\"Overall Accuracy: {report['unet']['metrics']['overall_metrics']['overall_accuracy']:.4f}\")\n",
    "        print(f\"Detection Rate: {report['unet']['metrics']['overall_metrics']['overall_detection_rate']:.4f}\")\n",
    "        print(f\"Mean Distance: {report['unet']['metrics']['overall_metrics']['overall_mean_distance']:.2f} pixels\")\n",
    "        print(f\"Total Samples: {report['unet']['metrics']['overall_metrics']['total_samples']}\")\n",
    "        print(f\"Total Landmarks: {report['unet']['metrics']['overall_metrics']['total_landmarks']}\")\n",
    "    else:\n",
    "        print(\"No U-Net metrics available\")\n",
    "    \n",
    "    print(f\"Inference Time: {report['unet']['performance']['mean_inference_time']:.4f} ± {report['unet']['performance']['std_inference_time']:.4f} seconds\")\n",
    "    print(f\"FPS: {report['unet']['performance']['fps']:.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Example usage (uncomment when all results are available)\n",
    "# performance_report = create_performance_report(retinanet_metrics, unet_metrics, times, unet_times)\n",
    "# print_performance_summary(performance_report)\n",
    "# save_results_to_csv(performance_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_inference_pipeline():\n",
    "    \"\"\"\n",
    "    Run complete inference pipeline for both models\n",
    "    \"\"\"\n",
    "    print(\"Starting complete inference pipeline...\")\n",
    "    \n",
    "    # 1. Load models\n",
    "    print(\"\\n1. Loading models...\")\n",
    "    # retinanet_model = load_retinanet_model(RETINANET_CONFIG['MODEL_PATH'], RETINANET_CONFIG['NUM_CLASSES'], device)\n",
    "    # unet_models = load_unet_models(UNET_CONFIG, device)\n",
    "    \n",
    "    # 2. Prepare test datasets\n",
    "    print(\"\\n2. Preparing test datasets...\")\n",
    "    # test_dataloader, test_dataset = create_retinanet_test_dataset(RETINANET_CONFIG, RETINANET_DATA_PATHS)\n",
    "    # x_test, y_test = prepare_unet_test_dataset(UNET_DATA_PATHS, UNET_CONFIG)\n",
    "    # test_loaders = create_unet_test_dataloaders(x_test, y_test, UNET_CONFIG)\n",
    "    \n",
    "    # 3. Run inference\n",
    "    print(\"\\n3. Running inference...\")\n",
    "    # retinanet_predictions, retinanet_ground_truths, retinanet_times = batch_inference_retinanet(retinanet_model, test_dataloader, device, RETINANET_CONFIG)\n",
    "    # unet_predictions, unet_ground_truths, unet_times = batch_inference_unet(unet_models, test_loaders, device, UNET_CONFIG)\n",
    "    \n",
    "    # 4. Evaluate results\n",
    "    print(\"\\n4. Evaluating results...\")\n",
    "    # retinanet_metrics = evaluate_retinanet_predictions(retinanet_predictions, retinanet_ground_truths)\n",
    "    # unet_metrics = evaluate_unet_predictions(unet_predictions, unet_ground_truths, UNET_CONFIG)\n",
    "    \n",
    "    # 5. Generate report\n",
    "    print(\"\\n5. Generating performance report...\")\n",
    "    # performance_report = create_performance_report(retinanet_metrics, unet_metrics, retinanet_times, unet_times)\n",
    "    # print_performance_summary(performance_report)\n",
    "    \n",
    "    # 6. Save results\n",
    "    print(\"\\n6. Saving results...\")\n",
    "    # save_results_to_csv(performance_report)\n",
    "    \n",
    "    # 7. Create visualizations\n",
    "    print(\"\\n7. Creating visualizations...\")\n",
    "    # visualize_retinanet_predictions(sample_images, retinanet_predictions[:4], retinanet_ground_truths[:4])\n",
    "    # visualize_unet_predictions(sample_images, unet_predictions[:4], unet_ground_truths[:4], UNET_CONFIG)\n",
    "    # plot_evaluation_metrics(retinanet_metrics, unet_metrics, UNET_CONFIG)\n",
    "    \n",
    "    print(\"\\n✅ Inference pipeline completed successfully!\")\n",
    "    \n",
    "    # return performance_report\n",
    "\n",
    "# Run the complete pipeline (uncomment when ready)\n",
    "# final_report = run_complete_inference_pipeline()\n",
    "\n",
    "print(\"\\n📋 USAGE INSTRUCTIONS:\")\n",
    "print(\"1. Update model paths in RETINANET_CONFIG and UNET_CONFIG\")\n",
    "print(\"2. Update dataset paths in RETINANET_DATA_PATHS and UNET_DATA_PATHS\")\n",
    "print(\"3. Uncomment the relevant sections to run inference\")\n",
    "print(\"4. Run run_complete_inference_pipeline() for end-to-end evaluation\")\n",
    "print(\"\\n🎯 This notebook provides:\")\n",
    "print(\"- Single image and batch inference for both models\")\n",
    "print(\"- Comprehensive evaluation metrics\")\n",
    "print(\"- Performance analysis and timing\")\n",
    "print(\"- Visualization of results\")\n",
    "print(\"- CSV export of results\")\n",
    "print(\"- Complete inference pipeline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}